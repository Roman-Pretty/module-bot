# Week 2 -The MapReduce programming model

# Introduction to MapReduce

## Overview

- MapReduce is “a simple and powerful **interface** that enables **automatic parallelisation and distribution of large-scale computations,** combined with an **implementation** of this interface that achieves high performance on large **clusters of commodity PCs**”
- More simply, MapReduce is a parallel **programming model** with Map and Reduce functions and their associated functions (Hadoop)

## The MapReduce Programming Model

A MapReduce job consists of two functions

- The Map function is called on an **input item** and emits intermediate **key/value** pairs
- The **reduce** function is called on groups of pairs with the **same key,** and emits the results for this key

Given a job, if we can express express it using a map function and a reduce function we are using the MapReduce programming model

## MapReduce parallelisation

MapReduce creates two opportunities for parallelisation

- Input data can be **partitioned into chunks,** each of which can be assigned to a different mapper
- The map stage produces collections of key/value pairs, that are grouped by key. Each **distinct key-group** can be sent to a different reducer

Map jobs are completed first, then a synchronisation step occurs and reduce jobs start

![image.png](image.png)

## How MapReduce parallelises

- Input data is **partitioned** into processable chunks/partitions
- **One Map Job is executed per chunk of data**
    - All mappers can be parallelised (depending on the number of nodes)
- **One Reduce Job is executed for each distinct key emitted by the Mappers**
    - All reducers can be parallelised (partitioned almost evenly among nodes)

Computing nodes first work on the Map Jobs. After all have completed the Map Jobs, a synchronisation step occurs, and then the computing nodes start running Reduce jobs

## MapReduce Benefits

- High level parallel programming abstraction
- Framework **implementations** provide good performance results
- Greatly reduces parallel programming **complexity**
- ***However,** it is not suitable for every parallel programming algorithm*

## Synchronisation Steps

- Every Key-Value item generated by the mappers is collected
- Items are transferred over the network (might be temporarily stored in memory/network storage)
- Same key items are grouped into a list of values
- Data is partitioned among the number of reducers (nearly even partitions by # of keys)
- Data is copied over the network to each reducer (if needed)
- **The data provided to each reducer is sorted according to the keys**

## MapReduce Runtime System

The runtime is responsible for the following

- Partitions input data
- Schedules execution across a set of machines
- Handles load balancing
- Shuffles, partitions and sorts data between Map and Reduce steps
- Handles machine failure transparently
- Manages inter process communication

# MapReduce Implementation

## Overview

The MapReduce programming model defines **two main functions,** namely, **Map** and **Reduce.** Both functions encapsulate the logic of the solution and can be parallelised by allocating them to different nodes.

However, in order for this strategy to work, in addition to expressing the logic of the solutions as map and reduce functions, we need to **partition the input data** and **partition the move key/value pairs** from mappers to reducers

**MapReduce specifies how this is implemented.** When creating a MapReduce job, we do not need to worry about implementing this: the **MapReduce framework will do it for us.**

## Shuffle and Sort

- At the end of the **map** stage:
    - All emitted key/value pairs are collected
    - Partitioned evenly
    - Each partition is sent to one reducer
- Reducers then:
    - Obtain their partitions from each mapper (shuffling)
    - Partitions from different mappers are grouped by key (sort). A group of key/value pairs with the same key is expressed as a single **key/value** pair, where the new **value is a list of all the input values**
    - The reduce function is called on this key/value pair

[https://www.notion.so](https://www.notion.so)

## Shuffle and Sort - On each Mapper

- All emitted key-value pairs are collected
    - In-memory buffer (100mb default size) then spills to hard disk
- Key-value pairs are partitioned depending on target reducer
    - Partitioning aims at **evenly** splitting keys
- Output is available to the reducers through HTTP server threads

## Shuffle and Sort - On each Reducer

- The reducer downloads output from mappers
    - Potentially all Mappers are contacted (communication bottleneck)
- Partial values from each mapper are merged (e.g. same key on different mappers)
- Keys are sorted and fed as input for the reducer
    - List of <Key, List<Value>>, sorted by key

## The Cost of Communications

Parallelising Map and Reduce jobs allows algorithms to scale **sub-linearly**

One potential bottleneck for MapReduce programs is the **cost of Shuffle and Sort** operations

- Data has to be **copied over network (expensive communication cost)**
    - All keys emitted by the mappers have to be transferred
- **Sorting** operation of large amounts of elements can also be costly

**Combiner** is an additional optional step that can be executed before reducer to **reduce the communication volume**

## The Combiner

- The combiner is a preliminary router
- It is executed at **each mapper node** just before sending all the key value pairs for shuffling
- Reduces the number of emitted items and improves efficiency
- It is **not mandatory** (the algorithm must work correctly if the combiner is not invoked)
- Nearly the same as the reducer function (but not always the case)

## Combiner Rules

The combiner has the same structure as the reducer (same input parameters) but must comply with these rules

- Idempotent - The number of times the combiner is applied cant change the output
- Transitive - The order of the inputs cant the output
- Side-effect free - Combiners cant have side effects (or they wont be idempotent)
- Preserves the sort order - They cant change keys to disrupt the sort order
- Preserves the partitioning - They cant change the keys to change the partitioning to the Reducers

# MapReduce Programming Patterns

## Overview

MapReduce provides a high-level abstraction:

- The primary task is to **express the logic** of our solution as a **map** function followed by a **reduce** function. This reduces parallel programming complexity.
- The **low-level implementation is common** to all MapReduce solutions. This gives an opportunity to optimise the low-level implementation and improve performance results

MapReduce is not a tool, but a **framework:** any solution has to be expressed using map and reduce functions. It turns out that **some problems are amenable to the MapReduce model, others are not**

A family of problems whose solution follows the same strategy to be implemented in the MapReduce framework is called a MapReduce programming pattern

## Pattern 1: Inverted Index

**Goal:** Generate index from a dataset to allow faster searches for specific features

Examples:

- Building index from a textbook
- Finding all websites that match a search term

**Inverted Index Structure:**

![image.png](image%201.png)

## Pattern 2: Filtering

**Goal:** Filter out records/fields that are not of interest for further computation.

Speedup the actual computation thanks to the reduced size of the dataset

Examples:

- Distributed *grep* (text patterns match)
- Tracking a thread of events (logs from the same user)
- Data cleansing

**Filtering is a mapper only job: it doesn’t produce an aggregation**

**Filtering Structure:**

![image.png](image%202.png)

## Pattern 3: Top-K

**Goal:** retrieve a small number of records, relative to a ranking

Examples:

- Build top sellers view
- Find outliers in data

In this pattern, only **one reducer** will be used. Hence, there is no need to perform partition and shuffling. **Sorting will be part of the reduce function.**

**Top Ten Pattern**

![image.png](image%203.png)

## Performance Considerations

- How many Mappers and Reducers instances?
- Performance issues for using too many?
- What happens if we don’t use combiners?

Performance depends greatly on the **number of elements,** (to a lesser extent on the **size of data**). For instance, data could have been filtered by the ingestion stage.

# Aggregate Computations

## Numerical Summarisation

**Goal:** Calculate aggregate statistical values over a dataset

Extract features from the dataset elements and compute the same statistical function for each feature.

Examples:

- Count occurrences
- Maximum / minimum values
- Average / median / standard deviation